
# SocratesEval: Benchmark for Logical Fallacy Understanding

This repository accompanies the paper:

**SOCRATESEVAL: Socrates or Smarty Pants? A Benchmark for Evaluating Large Language Models on Real-World Logical Fallacies*

üìÑ **Website:** [Welcome!](https://remarkably-mind-blowing-lab.github.io/smarty-pat-logic-bench/)

---

## üß† Overview

**SOCRATESEVAL** is a benchmark of **979 real-world, fallacy-rich questions from Reddit**, annotated with **15 logical fallacy types** and expert-written explanations. It evaluates **17 large language models (LLMs)** across three tasks:

- **Fallacy detection**
- **Fine-grained classification**
- **Explanation‚Äìhuman alignment**

Key findings reveal that there is **no correlation between correct classification and explanation quality**.

The dataset and code are **open-source**, with a **public leaderboard**.

---

## üì¶ Repository Structure

The repository is organized as follows:

### `/fig`

Contains resources used to generate visualizations such as bar charts and other figures.

### `/statistics`

Contains scripts for:
- Calculating **F1 scores**, **categorization metrics**, and **explanation scores**.
- Generating general statistics of the dataset.

### `/fallacy`

Contains all logic and explanation scripts organized per model:

- Each model has a folder under `/fallacy/<modelname>`.
- Each folder contains two types of Python files:
  - `logic_{model}.py`: Employs the specified LLM to **detect fallacies** and provide logical reasoning.
  - `explanation_{model}.py`: Prompts the LLM to generate **explanations** for each benchmark sentence, without explicit classification.

In addition, in the `/fallacy` root directory:
- `evaluator_{model}.py`: Evaluates how well the explanation generated by the model matches human-annotated explanations.
- `main.py`: Master script to run the full evaluation pipeline:
  1. Runs all `explanation_{model}.py` scripts,
  2. Runs `merge.py` to combine and organize explanation results,
  3. Runs all `evaluator_{model}.py` scripts to generate explanation scores.

### `/res`

Contains all experimental **results**, including generated outputs, evaluation scores, and metadata.

---

## ‚öôÔ∏è Environment Setup

To install dependencies and prepare the environment, run:

```bash
pip install -r requirements.txt
```

---
